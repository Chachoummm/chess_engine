{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnZwVAbeDiZLtg8zLCi4jJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chachoummm/chess_engine/blob/main/Train_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZnWtaJvcF5z"
      },
      "source": [
        "#libraries\n",
        "import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X8HHRWXdoUS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSYPigwUcGZs"
      },
      "source": [
        "\n",
        "# comme c'est trop lourd c'est pas sur github donc le modèle a été entrainé en local et puis on y a fait appel sur colab à partir de Github\n",
        "train = pd.read_csv('/content/gdrive/MyDrive/final_dataset.csv', index_col=None, header=0)\n",
        "#train = pd.concat(li, axis=0, ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW6ibqpjcGlt"
      },
      "source": [
        "train = shuffle(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5eog2BPcGs0",
        "outputId": "5bb64a4c-4452-4afc-cdf1-8b56b4a570d3"
      },
      "source": [
        "print (train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(610240, 193)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7bE1-bechww",
        "outputId": "883b0d0f-add4-4c6e-f854-9b917a76aace"
      },
      "source": [
        "print (train.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          a1    b1    c1    d1    e1  ... to_e8 to_f8 to_g8 to_h8 good_move\n",
            "166370  None     q  None  None  None  ...   0.0   0.0   0.0   0.0     False\n",
            "82799      R  None  None  None  None  ...   0.0   0.0   0.0   0.0      True\n",
            "440393  None     R     B     R  None  ...   0.0   0.0   0.0   0.0     False\n",
            "349346     R  None     B  None  None  ...   0.0   0.0   1.0   0.0     False\n",
            "473712     R     N     B     Q     K  ...   0.0   0.0   0.0   0.0     False\n",
            "\n",
            "[5 rows x 193 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz4p3ctwch5B"
      },
      "source": [
        "#features transformations \n",
        "features = list(train.iloc[:, 0:192].columns)\n",
        "X = train[features]\n",
        "y = train['good_move']\n",
        "categorical_columns = list(X.iloc[:, 0:63].columns)\n",
        "numerical_columns = list(X.iloc[:, 64:192].columns)\n",
        "feature_columns = []\n",
        "\n",
        "for feature_name in categorical_columns:\n",
        "  vocabulary = X[feature_name].unique()\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "\n",
        "for feature_name in numerical_columns:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name,dtype = tf.float32))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfM_nTAgch-Q"
      },
      "source": [
        "#input function :   \n",
        "  \n",
        "def make_input_fn(data_df, label_df, num_epochs = 10, shuffle = True, batch_size = 32):\n",
        "  def input_function():\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    return ds\n",
        "  return input_function "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOiYaWITciBd"
      },
      "source": [
        "#split data into batches :  \n",
        "  \n",
        "def split_into_batches(df, batch_size=100000):\n",
        "  nb_rows = len(df.index)\n",
        "  intervals = []\n",
        "  \n",
        "  for i in range(0, nb_rows + 1, batch_size):\n",
        "    intervals.append(i)\n",
        "  \n",
        "  if(intervals[-1] != nb_rows):\n",
        "    intervals.append(nb_rows)\n",
        "  \n",
        "  batches_X = []\n",
        "  batches_y = []\n",
        "  \n",
        "  for i in range(0, len(intervals) - 1):\n",
        "    batches_X.append(train.iloc[intervals[i]:intervals[i + 1], :][features])\n",
        "    batches_y.append(train.iloc[intervals[i]:intervals[i + 1], :]['good_move'])\n",
        "\n",
        "  return batches_X, batches_y\n",
        "\n",
        "batches_X, batches_y = split_into_batches(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djdqV5OMdCdD"
      },
      "source": [
        "#model : \n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns = feature_columns, model_dir='/content/gdrive/MyDrive/Saved_train_model',\n",
        "    warm_start_from=None, loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n",
        "    sparse_combiner='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnskrRO0dCkk",
        "outputId": "62112f4b-7bc8-4b10-c9b0-7123d939015f"
      },
      "source": [
        "#Create a list with all the input function : \n",
        "input_functions = []\n",
        "for df_X, df_y in zip(batches_X, batches_y):\n",
        "  input_functions.append(make_input_fn(df_X, df_y))\n",
        "\n",
        "print(len(input_functions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtPeJLG1dCrn"
      },
      "source": [
        "# train the model on all the input functions\n",
        "i = 1\n",
        "for input_function in input_functions:\n",
        "  print('<======================================== NEW BATCH ========================================>')\n",
        "  print('Batch: ' + str(i))\n",
        "  i = i + 1\n",
        "  linear_est.train(input_function)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbU8Vqrw9bHN"
      },
      "source": [
        "#Evaluate the model \n",
        "\n",
        "predict = \n",
        "# Evaluate the model on all the input functions\n",
        "i = 1\n",
        "for input_function in input_functions:\n",
        "  print('<======================================== NEW BATCH ========================================>')\n",
        "  print('Batch: ' + str(i))\n",
        "  i = i + 1\n",
        "  linear_est.train(input_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reCO2d-WdCw9"
      },
      "source": [
        "# save the model\n",
        "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "  tf.feature_column.make_parse_example_spec(feature_columns))\n",
        "\n",
        "estimator_base_path = '/content/gdrive/MyDrive/Saved_train_model'\n",
        "estimator_path = linear_est.export_saved_model(estimator_base_path, serving_input_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTuCfHvndC2X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}